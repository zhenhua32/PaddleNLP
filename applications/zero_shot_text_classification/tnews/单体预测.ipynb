{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 将模型转换成 ONNX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import onnx\n",
    "import onnxruntime as ort\n",
    "import paddle2onnx\n",
    "from onnxconverter_common import float16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "onnx_dir = r\"G:\\code\\github\\PaddleNLP\\outputs\\tnews\\export_model\\onnx\"\n",
    "os.makedirs(onnx_dir, exist_ok=True)\n",
    "\n",
    "float_onnx_file = os.path.join(onnx_dir, \"model.onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "_static_model_file = r\"G:\\code\\github\\PaddleNLP\\outputs\\tnews\\export_model\\model.pdmodel\"\n",
    "_static_params_file = r\"G:\\code\\github\\PaddleNLP\\outputs\\tnews\\export_model\\model.pdiparams\"\n",
    "\n",
    "onnx_model = paddle2onnx.command.c_paddle_to_onnx(\n",
    "    model_file=_static_model_file,\n",
    "    params_file=_static_params_file,\n",
    "    opset_version=13,\n",
    "    enable_onnx_checker=True,\n",
    ")\n",
    "with open(float_onnx_file, \"wb\") as f:\n",
    "    f.write(onnx_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 可能有数值溢出, fp16 精度不够\n",
    "# fp16_model_file = os.path.join(onnx_dir, \"fp16_model.onnx\")\n",
    "# onnx_model = onnx.load_model(float_onnx_file)\n",
    "# trans_model = float16.convert_float_to_float16(onnx_model, keep_io_types=True)\n",
    "# onnx.save_model(trans_model, fp16_model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = [(\"CUDAExecutionProvider\", {\"device_id\": 0})]\n",
    "sess_options = ort.SessionOptions()\n",
    "sess_options.intra_op_num_threads = os.cpu_count() // 2\n",
    "sess_options.inter_op_num_threads = os.cpu_count() // 2\n",
    "# 原来是在这里初始化 predictor 的, 而且用的是 fp16 版本的\n",
    "predictor = ort.InferenceSession(float_onnx_file, sess_options=sess_options, providers=providers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
      "['input_ids', 'token_type_ids', 'position_ids', 'attention_mask', 'omask_positions', 'cls_positions']\n"
     ]
    }
   ],
   "source": [
    "print(predictor.get_providers())\n",
    "\n",
    "input_handler = [i.name for i in predictor.get_inputs()]\n",
    "print(input_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 好了, 转换成 onnx 模型已经可以了, 现在是要准备输入数据了"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = ['故事', '文化', '娱乐', '体育', '财经', '房产', '汽车', '教育', '科技', '军事', '旅游', '国际', '股票', '农业', '电竞']\n",
    "data = {\n",
    "    \"text_a\": \"农村依然很重视土葬\",\n",
    "    \"text_b\": \"\",\n",
    "    \"choices\": schema,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-07-26 20:11:26,139] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'G:\\code\\github\\PaddleNLP\\outputs\\tnews'.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers import UTC, AutoTokenizer\n",
    "from paddlenlp.prompt import PromptDataCollatorWithPadding, UTCTemplate\n",
    "\n",
    "task_path = r\"G:\\code\\github\\PaddleNLP\\outputs\\tnews\"\n",
    "max_seq_len = 512\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(task_path)\n",
    "collator = PromptDataCollatorWithPadding(tokenizer, return_tensors=\"np\")\n",
    "template = UTCTemplate(tokenizer, max_seq_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = [data]\n",
    "tokenized_inputs = [template(i) for i in inputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['soft_token_ids', 'input_ids', 'position_ids', 'token_type_ids', 'attention_mask', 'choices', 'omask_positions', 'cls_positions'])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized_inputs[0].keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = {}\n",
    "outputs[\"text\"] = inputs\n",
    "# 这里会调用 _collator, 转换类型成 np\n",
    "outputs[\"batches\"] = collator(tokenized_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtype_dict = {\n",
    "    \"input_ids\": \"int64\",\n",
    "    \"token_type_ids\": \"int64\",\n",
    "    \"position_ids\": \"int64\",\n",
    "    \"attention_mask\": \"float32\",\n",
    "    \"omask_positions\": \"int64\",\n",
    "    \"cls_positions\": \"int64\",\n",
    "}\n",
    "\n",
    "outputs[\"batch_logits\"] = []\n",
    "# onnx 推理\n",
    "batch = outputs[\"batches\"]\n",
    "input_dict = {}\n",
    "for input_name in dtype_dict:\n",
    "    input_dict[input_name] = batch[input_name].astype(dtype_dict[input_name])\n",
    "logits = predictor.run(None, input_dict)[0].tolist()\n",
    "# 最后就是添加 logits\n",
    "outputs[\"batch_logits\"] = logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.218139410018921,\n",
       "  -2.090022325515747,\n",
       "  -6.312975883483887,\n",
       "  -8.242919921875,\n",
       "  -6.24294900894165,\n",
       "  -5.859382629394531,\n",
       "  -6.72791051864624,\n",
       "  -6.126931190490723,\n",
       "  -6.983493328094482,\n",
       "  -5.1142659187316895,\n",
       "  -3.1050539016723633,\n",
       "  -7.003054141998291,\n",
       "  -8.959622383117676,\n",
       "  0.9350083470344543,\n",
       "  -6.280238151550293]]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"batch_logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[-1.218139410018921,\n",
       "  -2.090022325515747,\n",
       "  -6.312975883483887,\n",
       "  -8.242919921875,\n",
       "  -6.24294900894165,\n",
       "  -5.859382629394531,\n",
       "  -6.72791051864624,\n",
       "  -6.126931190490723,\n",
       "  -6.983493328094482,\n",
       "  -5.1142659187316895,\n",
       "  -3.1050539016723633,\n",
       "  -7.003054141998291,\n",
       "  -8.959622383117676,\n",
       "  0.9350083470344543,\n",
       "  -6.280238151550293]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "outputs[\"batch_logits\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import expit as np_sigmoid\n",
    "from scipy.special import softmax as np_softmax\n",
    "import numpy as np\n",
    "pred_threshold = 0.5\n",
    "single_label = True\n",
    "\n",
    "result = []\n",
    "for text, logits in zip(outputs[\"text\"], outputs[\"batch_logits\"]):\n",
    "    # 重新构建每个输出\n",
    "    output = {}\n",
    "    if len(text[\"text_a\"]) > 0:\n",
    "        output[\"text_a\"] = text[\"text_a\"]\n",
    "    if len(text[\"text_b\"]) > 0:\n",
    "        output[\"text_b\"] = text[\"text_b\"]\n",
    "    \n",
    "    # 单标签\n",
    "    if single_label:\n",
    "        score = np_softmax(logits, axis=-1)\n",
    "        label = np.argmax(logits, axis=-1)\n",
    "        output[\"predictions\"] = [{\"label\": text[\"choices\"][label], \"score\": score[label]}]\n",
    "    else:\n",
    "        scores = np_sigmoid(logits)\n",
    "        output[\"predictions\"] = []\n",
    "        if scores.ndim == 2:\n",
    "            scores = scores[0]\n",
    "        for i, class_score in enumerate(scores):\n",
    "            if class_score > pred_threshold:\n",
    "                output[\"predictions\"].append({\"label\": text[\"choices\"][i], \"score\": class_score})\n",
    "    result.append(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'text_a': '农村依然很重视土葬',\n",
       "  'predictions': [{'label': '农业', 'score': 0.84022164792424}]}]"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

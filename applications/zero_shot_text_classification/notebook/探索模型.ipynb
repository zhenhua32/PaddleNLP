{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 看看模型和结构和输入输出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import sys\n",
    "from pprint import pprint\n",
    "import numpy as np\n",
    "\n",
    "# 尝试覆盖掉已经安装的 paddlenlp\n",
    "sys.path.insert(0, r\"G:\\code\\github\\PaddleNLP\")\n",
    "\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.prompt import (\n",
    "    PromptModelForSequenceClassification,\n",
    "    PromptTrainer,\n",
    "    PromptTuningArguments,\n",
    "    UTCTemplate,\n",
    ")\n",
    "from paddlenlp.transformers import UTC, AutoTokenizer, export_model\n",
    "\n",
    "sys.path.append(r\"G:\\code\\github\\PaddleNLP\\applications\\zero_shot_text_classification\")\n",
    "from utils import UTCLoss, read_local_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_dir = \"utc-base\"\n",
    "model_dir = \"utc-base\"\n",
    "data_dir = r\"G:\\dataset\\text_classify\\baidu_utc_medical\\raw\"\n",
    "train_file = \"train.txt\"\n",
    "dev_file = \"dev.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-07-15 09:10:55,207] [    INFO]\u001b[0m - We are using (<class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'>, False) to load 'utc-base'.\u001b[0m\n",
      "\u001b[32m[2023-07-15 09:10:55,208] [    INFO]\u001b[0m - Already cached C:\\Users\\zhenh\\.paddlenlp\\models\\utc-base\\utc_base_vocab.txt\u001b[0m\n",
      "\u001b[32m[2023-07-15 09:10:55,223] [    INFO]\u001b[0m - tokenizer config file saved in C:\\Users\\zhenh\\.paddlenlp\\models\\utc-base\\tokenizer_config.json\u001b[0m\n",
      "\u001b[32m[2023-07-15 09:10:55,224] [    INFO]\u001b[0m - Special tokens file saved in C:\\Users\\zhenh\\.paddlenlp\\models\\utc-base\\special_tokens_map.json\u001b[0m\n",
      "\u001b[32m[2023-07-15 09:10:56,025] [    INFO]\u001b[0m - All model checkpoint weights were used when initializing UTC.\n",
      "\u001b[0m\n",
      "\u001b[32m[2023-07-15 09:10:56,025] [    INFO]\u001b[0m - All the weights of UTC were initialized from the model checkpoint at utc-base.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use UTC for predictions without further training.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp.transformers.ernie.tokenizer import ErnieTokenizer\n",
    "\n",
    "# Load the pretrained language model. 加载模型和分词器\n",
    "tokenizer: ErnieTokenizer = AutoTokenizer.from_pretrained(model_dir)\n",
    "model = UTC.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-07-15 08:50:11,392] [    INFO]\u001b[0m - Assigning ['[O-MASK]'] to the additional_special_tokens key of the tokenizer\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Define template for preprocess and verbalizer for postprocess. 看看这个模板是怎么生成的\n",
    "template = UTCTemplate(tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m[2023-07-15 08:52:29,140] [ WARNING]\u001b[0m - Skip 0 examples.\u001b[0m\n",
      "\u001b[33m[2023-07-15 08:52:29,141] [ WARNING]\u001b[0m - Skip 0 examples.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "# Load and preprocess dataset. 加载数据集\n",
    "train_ds = load_dataset(\n",
    "    read_local_dataset,\n",
    "    data_path=data_dir,\n",
    "    data_file=train_file,\n",
    "    lazy=False,\n",
    ")\n",
    "dev_ds = load_dataset(\n",
    "    read_local_dataset,\n",
    "    data_path=data_dir,\n",
    "    data_file=dev_file,\n",
    "    lazy=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45\n",
      "{'choices': ['病情诊断',\n",
      "             '治疗方案',\n",
      "             '病因分析',\n",
      "             '指标解读',\n",
      "             '就医建议',\n",
      "             '疾病表述',\n",
      "             '后果表述',\n",
      "             '注意事项',\n",
      "             '功效作用',\n",
      "             '医疗费用',\n",
      "             '其他'],\n",
      " 'labels': [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0],\n",
      " 'question': '',\n",
      " 'text_a': '糖尿病蜜月期有永久性的吗',\n",
      " 'text_b': ''}\n",
      "疾病表述\n"
     ]
    }
   ],
   "source": [
    "print(len(train_ds))\n",
    "pprint(train_ds[0])\n",
    "for label, index in zip(train_ds[0][\"choices\"], train_ds[0][\"labels\"]):\n",
    "    if index == 1:\n",
    "        print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the criterion. 定义损失\n",
    "criterion = UTCLoss()\n",
    "\n",
    "# Initialize the prompt model. 初始化提示学习的模型, 没有用到第三个参数 verbalizer\n",
    "prompt_model = PromptModelForSequenceClassification(\n",
    "    model, template, None, freeze_plm=False, freeze_dropout=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练时的输入字段"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "soft_token_ids <class 'list'> [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "input_ids <class 'list'> [1, 39980, 396, 182, 1395, 533, 39980, 301, 641, 58, 487, 39980, 396, 196, 59, 764, 39980, 288, 275, 273, 555, 39980, 113, 386, 81, 454, 39980, 1095, 396, 197, 779, 39980, 49, 228, 197, 779, 39980, 490, 221, 104, 236, 39980, 369, 344, 25, 29, 39980, 386, 641, 453, 29, 39980, 63, 44, 2, 1237, 1601, 396, 2027, 136, 195, 9, 885, 876, 92, 5, 1114, 2, 2]\n",
      "69\n",
      "['[CLS]', '[O-MASK]', '病', '情', '诊', '断', '[O-MASK]', '治', '疗', '方', '案', '[O-MASK]', '病', '因', '分', '析', '[O-MASK]', '指', '标', '解', '读', '[O-MASK]', '就', '医', '建', '议', '[O-MASK]', '疾', '病', '表', '述', '[O-MASK]', '后', '果', '表', '述', '[O-MASK]', '注', '意', '事', '项', '[O-MASK]', '功', '效', '作', '用', '[O-MASK]', '医', '疗', '费', '用', '[O-MASK]', '其', '他', '[SEP]', '糖', '尿', '病', '蜜', '月', '期', '有', '永', '久', '性', '的', '吗', '[SEP]', '[SEP]']\n",
      "[CLS] [O-MASK] 病 情 诊 断 [O-MASK] 治 疗 方 案 [O-MASK] 病 因 分 析 [O-MASK] 指 标 解 读 [O-MASK] 就 医 建 议 [O-MASK] 疾 病 表 述 [O-MASK] 后 果 表 述 [O-MASK] 注 意 事 项 [O-MASK] 功 效 作 用 [O-MASK] 医 疗 费 用 [O-MASK] 其 他 [SEP] 糖 尿 病 蜜 月 期 有 永 久 性 的 吗 [SEP] [SEP]\n",
      "\n",
      "position_ids <class 'list'> [0, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 3, 4, 0, 1, 2, 0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 0]\n",
      "\n",
      "token_type_ids <class 'list'> [0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
      "\n",
      "attention_mask <class 'numpy.ndarray'> [[0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " ...\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]\n",
      " [0. 0. 0. ... 0. 0. 0.]]\n",
      "\n",
      "question <class 'str'> \n",
      "\n",
      "choices <class 'list'> ['病情诊断', '治疗方案', '病因分析', '指标解读', '就医建议', '疾病表述', '后果表述', '注意事项', '功效作用', '医疗费用', '其他']\n",
      "\n",
      "labels <class 'list'> [0.0, 0.0, 0.0, 0.0, 0.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "\n",
      "omask_positions <class 'list'> [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51]\n",
      "\n",
      "cls_positions <class 'int'> 54\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 训练时的输入字段\n",
    "for key, val in template(train_ds[0]).items():\n",
    "    print(key, type(val), val)\n",
    "\n",
    "    if key == \"input_ids\":\n",
    "        print(len(val))\n",
    "        print(tokenizer.convert_ids_to_tokens(val))\n",
    "        print(tokenizer.decode(val))\n",
    "        input_ids = val\n",
    "    if key == \"attention_mask\":\n",
    "        attention_mask = val\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, -10000.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n",
      "[0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0]\n"
     ]
    }
   ],
   "source": [
    "for index in range(attention_mask.shape[0]):\n",
    "    print(attention_mask[index].tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "omask_index [1, 6, 11, 16, 21, 26, 31, 36, 41, 46, 51]\n",
      "cls_indices [0]\n",
      "sep_indices [54 67 68]\n",
      "cls_index 69\n",
      "sep_index 54\n",
      "1 54\n",
      "1 6\n",
      "6 11\n",
      "11 16\n",
      "16 21\n",
      "21 26\n",
      "26 31\n",
      "31 36\n",
      "36 41\n",
      "41 46\n",
      "46 51\n",
      "51 54\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "创建注意力掩码\n",
    "\"\"\"\n",
    "omask_token = \"[O-MASK]\"\n",
    "omask_id = tokenizer.convert_tokens_to_ids(omask_token)\n",
    "input_ids = np.array(input_ids)\n",
    "# 这是个矩阵, shape 是 [len(input_ids), len(input_ids)]\n",
    "attention_mask = np.ones([len(input_ids), len(input_ids)])\n",
    "# 都取 [0] 是因为 where 会返回一个元组\n",
    "omask_index = np.where(input_ids == omask_id)[0].tolist()\n",
    "cls_indices = np.where(input_ids == tokenizer.cls_token_id)[0]\n",
    "sep_indices = np.where(input_ids == tokenizer.sep_token_id)[0]\n",
    "\n",
    "print(\"omask_index\", omask_index)\n",
    "print(\"cls_indices\", cls_indices)\n",
    "print(\"sep_indices\", sep_indices)\n",
    "\n",
    "# 找一个最小的且在 omask_index 之后的 cls_index 和 sep_index\n",
    "cls_index = len(input_ids)\n",
    "for idx in cls_indices:\n",
    "    if idx > omask_index[-1]:\n",
    "        cls_index = idx\n",
    "        break\n",
    "print(\"cls_index\", cls_index)\n",
    "sep_index = len(input_ids)\n",
    "for idx in sep_indices:\n",
    "    if idx > omask_index[-1]:\n",
    "        sep_index = idx\n",
    "        break\n",
    "print(\"sep_index\", sep_index)\n",
    "# opt 的开始和结束位置\n",
    "opt_begin = omask_index[0]\n",
    "opt_end = min(cls_index, sep_index)\n",
    "print(opt_begin, opt_end)\n",
    "# 整一段都要忽略\n",
    "attention_mask[opt_begin:opt_end, opt_begin:opt_end] = 0\n",
    "omask_index.append(opt_end)\n",
    "for opt_begin, opt_end in zip(omask_index[:-1], omask_index[1:]):\n",
    "    print(opt_begin, opt_end)\n",
    "    # 其中是选项文本的部分要保留\n",
    "    attention_mask[opt_begin:opt_end, opt_begin:opt_end] = 1\n",
    "# 这个就是将 attention_mask 中的 0 变成 -1e4, 1 变成 0\n",
    "attention_mask = (attention_mask - 1) * 1e4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模型架构"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UTCTemplate()"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "UTC(\n",
       "  (ernie): ErnieModel(\n",
       "    (embeddings): ErnieEmbeddings(\n",
       "      (word_embeddings): Embedding(39981, 768, padding_idx=0, sparse=False)\n",
       "      (position_embeddings): Embedding(2048, 768, sparse=False)\n",
       "      (token_type_embeddings): Embedding(4, 768, sparse=False)\n",
       "      (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "      (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "    )\n",
       "    (encoder): TransformerEncoder(\n",
       "      (layers): LayerList(\n",
       "        (0): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (1): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (2): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (3): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (4): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (5): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (6): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (7): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (8): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (9): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (10): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "        (11): TransformerEncoderLayer(\n",
       "          (self_attn): MultiHeadAttention(\n",
       "            (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "          )\n",
       "          (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "          (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "          (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "          (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "          (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): ErniePooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (linear_q): Linear(in_features=768, out_features=64, dtype=float32)\n",
       "  (linear_k): Linear(in_features=768, out_features=64, dtype=float32)\n",
       ")"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 用的 utc-base 是 12 层的\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PromptModelForSequenceClassification(\n",
       "  (plm): UTC(\n",
       "    (ernie): ErnieModel(\n",
       "      (embeddings): ErnieEmbeddings(\n",
       "        (word_embeddings): Embedding(39981, 768, padding_idx=0, sparse=False)\n",
       "        (position_embeddings): Embedding(2048, 768, sparse=False)\n",
       "        (token_type_embeddings): Embedding(4, 768, sparse=False)\n",
       "        (layer_norm): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "        (dropout): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "      )\n",
       "      (encoder): TransformerEncoder(\n",
       "        (layers): LayerList(\n",
       "          (0): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (1): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (2): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (3): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (4): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (5): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (6): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (7): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (8): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (9): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (10): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "          (11): TransformerEncoderLayer(\n",
       "            (self_attn): MultiHeadAttention(\n",
       "              (q_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (k_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (v_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "              (out_proj): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "            )\n",
       "            (linear1): Linear(in_features=768, out_features=3072, dtype=float32)\n",
       "            (dropout): Dropout(p=0, axis=None, mode=upscale_in_train)\n",
       "            (linear2): Linear(in_features=3072, out_features=768, dtype=float32)\n",
       "            (norm1): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (norm2): LayerNorm(normalized_shape=[768], epsilon=1e-12)\n",
       "            (dropout1): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "            (dropout2): Dropout(p=0.1, axis=None, mode=upscale_in_train)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (pooler): ErniePooler(\n",
       "        (dense): Linear(in_features=768, out_features=768, dtype=float32)\n",
       "        (activation): Tanh()\n",
       "      )\n",
       "    )\n",
       "    (linear_q): Linear(in_features=768, out_features=64, dtype=float32)\n",
       "    (linear_k): Linear(in_features=768, out_features=64, dtype=float32)\n",
       "  )\n",
       "  (template): UTCTemplate()\n",
       ")"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 组合了 UTC 模型和 template\n",
    "prompt_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 预测\n",
    "\n",
    "如果使用 fp16, 需要安装这些 onnx 相关的依赖\n",
    "\n",
    "```\n",
    "onnx                     1.14.0\n",
    "onnxconverter-common     1.13.0\n",
    "onnxruntime-gpu          1.15.1\n",
    "paddle2onnx              1.0.6\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[32m[2023-07-16 13:46:13,850] [    INFO]\u001b[0m - We are using <class 'paddlenlp.transformers.ernie.tokenizer.ErnieTokenizer'> to load 'G:\\code\\github\\PaddleNLP\\outputs\\plm'.\u001b[0m\n",
      "\u001b[32m[2023-07-16 13:46:13,876] [    INFO]\u001b[0m - Assigning ['[O-MASK]'] to the additional_special_tokens key of the tokenizer\u001b[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'predictions': [{'label': '后果表述', 'score': 0.9999968636902635}],\n",
      "  'text_a': '月经期间刮痧拔罐会引起身体什么'}]\n"
     ]
    }
   ],
   "source": [
    "from paddlenlp import Taskflow\n",
    "\n",
    "schema = [\"病情诊断\", \"治疗方案\", \"病因分析\", \"指标解读\", \"就医建议\", \"疾病表述\", \"后果表述\", \"注意事项\", \"功效作用\", \"医疗费用\", \"其他\"]\n",
    "my_cls = Taskflow(\"zero_shot_text_classification\", model=\"utc-base\", schema=schema, task_path=r\"G:\\code\\github\\PaddleNLP\\outputs\\plm\", precision=\"fp32\")\n",
    "task_instance = my_cls.task_instance\n",
    "pprint(my_cls(\"月经期间刮痧拔罐会引起身体什么\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "onnx                     1.14.0\n",
      "onnxconverter-common     1.13.0\n",
      "onnxruntime-gpu          1.15.1\n",
      "paddle2onnx              1.0.6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Ignoring invalid distribution -rotobuf (g:\\code\\github\\paddlenlp\\venv\\lib\\site-packages)\n"
     ]
    }
   ],
   "source": [
    "!pip list | findstr onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(True, False, 'paddle-inference', 12)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "task_instance._custom_model, task_instance.is_static_model, task_instance._predictor_type, task_instance._num_threads"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "paddle.fluid.libpaddle.PaddleInferPredictor"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(task_instance.predictor)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
